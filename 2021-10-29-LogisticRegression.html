<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Mathematics behind Logistic Regression (Binomial) &mdash; first_attempt 1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> first_attempt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Mathematics behind Logistic Regression (Binomial)</a><ul>
<li><a class="reference internal" href="#Mathematical-Representation">Mathematical Representation</a></li>
<li><a class="reference internal" href="#Coefficient-Estimates-by-MLE">Coefficient Estimates by MLE</a><ul>
<li><a class="reference internal" href="#Form-of-the-Log-Likelihood">Form of the Log Likelihood</a></li>
<li><a class="reference internal" href="#Deriving-the-First-Derivative">Deriving the First Derivative</a></li>
<li><a class="reference internal" href="#Deriving-the-Hessian-Matrix">Deriving the Hessian Matrix</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">first_attempt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Mathematics behind Logistic Regression (Binomial)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/2021-10-29-LogisticRegression.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Mathematics-behind-Logistic-Regression-(Binomial)">
<h1>Mathematics behind Logistic Regression (Binomial)<a class="headerlink" href="#Mathematics-behind-Logistic-Regression-(Binomial)" title="Permalink to this headline"></a></h1>
<blockquote>
<div><p>Learn how Logistic Regression is different from Linear Regression; formulate the problem, dervie the Hessian matrix and show that the problem is negative definite.</p>
</div></blockquote>
<ul class="simple">
<li><p>toc:true</p></li>
<li><p>badges:true</p></li>
<li><p>comments:true</p></li>
<li><p>categories:[Logistic Regression, MLE, Hessian Matrix]</p></li>
<li><p>image: images/Logistic_Regression.png</p></li>
</ul>
<p>Let <span class="math notranslate nohighlight">\(\vec{x} = (x_1, x_2, x_3, ..., x_p)^T\)</span> is the p dimensional independent vector and <span class="math notranslate nohighlight">\(y\)</span> is the categorical dependent variable. For now suppose <span class="math notranslate nohighlight">\(y\)</span> has only two categories (for more than two categories mathematics is little different). We will always represent categories by 0 and 1. In some cases to do that we may have to use appropriate technic to represent categories by 0 and 1. For example, if categories are boy or girl then we can represent 0 for boy and 1 for girl,
alternatively 0 for girl and 1 for boy.</p>
<section id="Mathematical-Representation">
<h2>Mathematical Representation<a class="headerlink" href="#Mathematical-Representation" title="Permalink to this headline"></a></h2>
<p>Now,we may try to fit a linear regression model of the form, <span class="math notranslate nohighlight">\(y_i=\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + ... + \beta_p x_{ip} + \epsilon_i\)</span> then, as all <span class="math notranslate nohighlight">\(y\)</span>s are not same, mix of 0 and 1, all the coefficient estimates can not be zero at the same time; ie. the regression plane would have non-zero slope along some axis. So, for suitable choice of <span class="math notranslate nohighlight">\(x\)</span> vector predicted value of y would be greater than 0, similarly for suitable choice of x predicted value of
y can be negative (becomes unbounded briefly said). So, what we do is, we set,</p>
<div class="math notranslate nohighlight">
\[y_i \sim Bernoulli(p_i)\]</div>
<p>, where,</p>
<div class="math notranslate nohighlight">
\[p_i = P(y_i = 1) = \frac{1}{1+ exp[-(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + ... + \beta_p x_{ip})]}\]</div>
<p>and estimate the parameters by <strong>MLE</strong>.</p>
</section>
<section id="Coefficient-Estimates-by-MLE">
<h2>Coefficient Estimates by MLE<a class="headerlink" href="#Coefficient-Estimates-by-MLE" title="Permalink to this headline"></a></h2>
<section id="Form-of-the-Log-Likelihood">
<h3>Form of the Log Likelihood<a class="headerlink" href="#Form-of-the-Log-Likelihood" title="Permalink to this headline"></a></h3>
<p>Likelihood,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
L &amp;= \prod_{i=1}^nP(y_i) \\
  &amp;= \prod_{i=1}^np_i^{y_i}(1 - p_i)^{1 - y_i}
\end{aligned}\end{split}\]</div>
<p>So, log-likelihood,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
l &amp;= \log L \\
  &amp;= \log \prod_{i=1}^np_i^{y_i}(1 - p_i)^{1 - y_i} \\
  &amp;= \sum_{i=1}^n y_i\log p_i + (1-y_i)\log(1-p_i) \\
  &amp;= \sum_{i=1}^n y_i.\log \frac{1}{1+ exp[-\vec{\beta}^T.\vec{x}_i]} + (1-y_i).\log \frac{exp[-\vec{\beta}^T.\vec{x}_i]}{1+ exp[-\vec{\beta}^T.\vec{x}_i]} \\
  &amp;= \sum_{i=1}^n -y_i.\log (1+ exp[-\vec{\beta}^T.\vec{x}_i]) + (1-y_i).\log exp[-\vec{\beta}^T.\vec{x}_i] \ - (1-y_i).\log (1+ exp[-\vec{\beta}^T.\vec{x}_i])\\
  &amp;= \sum_{i=1}^n -y_i.\log (1+ exp[-\vec{\beta}^T.\vec{x}_i]) - (1-y_i).\vec{\beta}^T.\vec{x}_i \ - (1-y_i).\log (1+ exp[-\vec{\beta}^T.\vec{x}_i])\\
  &amp;= \sum_{i=1}^n - (1-y_i).\vec{\beta}^T.\vec{x}_i \ - \log (1+ exp[-\vec{\beta}^T.\vec{x}_i])\\
  &amp;= -\sum_{i=1}^n (1-y_i).\vec{\beta}^T.\vec{x}_i \ + \log (1+ exp[-\vec{\beta}^T.\vec{x}_i])
\end{aligned}\end{split}\]</div>
<p>We will get the solution by solving <span class="math notranslate nohighlight">\(\frac{\partial l}{\partial \vec{\beta}} = 0\)</span>;</p>
</section>
<section id="Deriving-the-First-Derivative">
<h3>Deriving the First Derivative<a class="headerlink" href="#Deriving-the-First-Derivative" title="Permalink to this headline"></a></h3>
<p>Now,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial l}{\partial \vec{\beta}} &amp;= \frac{\partial}{\partial \vec{\beta}} -\sum_{i=1}^n (1-y_i).\vec{\beta}^T.\vec{x}_i \ + \log (1+ exp[-\vec{\beta}^T.\vec{x}_i]) \\
 &amp;= -\sum_{i=1}^n (1-y_i).\frac{\partial}{\partial \vec{\beta}} \vec{\beta}^T.\vec{x}_i \ + \frac{\partial}{\partial \vec{\beta}}\log (1+ exp[-\vec{\beta}^T.\vec{x}_i]) \\
 &amp;= -\sum_{i=1}^n (1-y_i).\vec{x}_i \ + \frac{1}{1+ exp[-\vec{\beta}^T.\vec{x}_i]}.exp[-\vec{\beta}^T.\vec{x}_i].(-1).\vec{x}_i \\
 &amp;= -\sum_{i=1}^n (1-y_i).\vec{x}_i \ + \sum_{i=1}^n \frac{exp[-\vec{\beta}^T.\vec{x}_i]}{1+ exp[-\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i \\
 &amp;= -\sum_{i=1}^n (1-y_i).\vec{x}_i \ + \sum_{i=1}^n \frac{1}{1+ exp[\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i
\end{aligned}\end{split}\]</div>
<p>Now, <span class="math notranslate nohighlight">\(\frac{\partial l}{\partial \beta} = 0\)</span> will produce global maximum, if we prove that <span class="math notranslate nohighlight">\(l\)</span> is concave function with respect to the model coefficients, ie. the <strong>Hessian Matrix</strong> is negative definite.</p>
<div class="math notranslate nohighlight">
\[H = \begin{pmatrix}\begin{pmatrix}\frac{\partial^2 l}{\partial \beta_n \beta_m}\end{pmatrix}\end{pmatrix} = \frac{\partial }{\partial \vec{\beta}}\left[\frac{\partial l}{\partial \vec{\beta}}\right]^T &lt; 0\]</div>
</section>
<section id="Deriving-the-Hessian-Matrix">
<h3>Deriving the Hessian Matrix<a class="headerlink" href="#Deriving-the-Hessian-Matrix" title="Permalink to this headline"></a></h3>
<p>Here,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
H &amp;= \frac{\partial }{\partial \vec{\beta}}\left[\frac{\partial l}{\partial \vec{\beta}}\right]^T \\
 &amp;= \frac{\partial }{\partial \vec{\beta}} \left[-\sum_{i=1}^n (1-y_i).\vec{x}_i \ + \sum_{i=1}^n \frac{1}{1+ exp[\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i \right]^T \\
 &amp;= -\frac{\partial }{\partial \vec{\beta}} \sum_{i=1}^n (1-y_i).\vec{x}_i^T  \ + \frac{\partial }{\partial \vec{\beta}}\sum_{i=1}^n \frac{1}{1+ exp[\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i^T \\
 &amp;= 0 \ \ + \sum_{i=1}^n \left[\frac{\partial }{\partial \vec{\beta}} \frac{1}{1+ exp[\vec{\beta}^T.\vec{x}_i]}\right].\vec{x}_i^T \\
 &amp;= \sum_{i=1}^n \left[ \frac{-1}{\left(1+ exp[\vec{\beta}^T.\vec{x}_i]\right)^2}.exp[\vec{\beta}^T.\vec{x}_i].\vec{x}_i\right]\vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n \frac{1}{1+ exp[\vec{\beta}^T.\vec{x}_i]}.\frac{exp[\vec{\beta}^T.\vec{x}_i]}{1+ exp[\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i.\vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n \frac{exp[-\vec{\beta}^T.\vec{x}_i]}{1+ exp[-\vec{\beta}^T.\vec{x}_i]}.\frac{1}{1+ exp[-\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i.\vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n p_i.(1-p_i).\vec{x}_i.\vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n \sqrt{p_i.(1-p_i)}.\vec{x}_i.\sqrt{p_i.(1-p_i)}.\vec{x}_i^T \\
 &amp;= -\sum_{i=1}^n \vec{x^*}_i .\vec{x^*}_i^T \ \ \leq \ \ 0
\end{aligned}\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\vec{x^*}_i = \sqrt{p_i.(1-p_i)}.\vec{x}_i\)</span>. The last equality holds for the fact that <span class="math notranslate nohighlight">\(\vec{x^*}_i .\vec{x^*}_i^T\)</span> is positive definite and sum of positive definite metrices is positive definite.</p>
<p>Now if we set <span class="math notranslate nohighlight">\(\frac{\partial l}{\partial \beta_m} = 0\)</span>, we cannot get explicit form of <span class="math notranslate nohighlight">\(\beta\)</span> due the complicated form of <span class="math notranslate nohighlight">\(\frac{\partial l}{\partial \beta_m}\)</span>, so to get the solution we have to use numerical methods.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Kaustav.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>